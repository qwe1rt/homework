{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd7c4526-3498-4f24-898a-a4bd741b3137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth: 1, Recall: 0.7111111111111111, Precision: 0.8555555555555555, F1-score: 0.6148148148148148\n",
      "Depth: 2, Recall: 1.0, Precision: 1.0, F1-score: 1.0\n",
      "Depth: 3, Recall: 1.0, Precision: 1.0, F1-score: 1.0\n",
      "Depth: 4, Recall: 1.0, Precision: 1.0, F1-score: 1.0\n",
      "Depth: 5, Recall: 1.0, Precision: 1.0, F1-score: 1.0\n",
      "Micro-average: Calculate the true positives, false positives, and false negatives for all classes and then compute the overall precision, recall, and F1-score.\n",
      "Macro-average: Calculate the precision, recall, and F1-score for each class separately and then take the average.\n",
      "Weighted-average: Calculate the precision, recall, and F1-score for each class separately and then perform a weighted average based on the number of samples in each class.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Decision trees with different depths\n",
    "depths = range(1, 6)\n",
    "for depth in depths:\n",
    "    clf = DecisionTreeClassifier(min_samples_leaf=2, min_samples_split=5, max_depth=depth)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    # Set the zero_division parameter\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    print(f\"Depth: {depth}, Recall: {recall}, Precision: {precision}, F1-score: {f1}\")\n",
    "\n",
    "# Differences among micro-average, macro-average, and weighted-average\n",
    "print(\"Micro-average: Calculate the true positives, false positives, and false negatives for all classes and then compute the overall precision, recall, and F1-score.\")\n",
    "print(\"Macro-average: Calculate the precision, recall, and F1-score for each class separately and then take the average.\")\n",
    "print(\"Weighted-average: Calculate the precision, recall, and F1-score for each class separately and then perform a weighted average based on the number of samples in each class.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de764a2e-5c1b-4b5c-8f5a-fd5c1bdc0c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of the first split: 0.0\n",
      "Gini index of the first split: 0.0\n",
      "Misclassification error of the first split: 0.0\n",
      "Information gain: 0.9340026588217948\n",
      "Feature selected for the first split: uniformity_of_cell_size\n",
      "Value of the decision boundary: 2.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data'\n",
    "column_names = ['id', 'clump_thickness', 'uniformity_of_cell_size', 'uniformity_of_cell_shape',\n",
    "                'marginal_adhesion','single_epithelial_cell_size', 'bare_nuclei',\n",
    "                'bland_chromatin', 'normal_nucleoli','mitoses', 'class']\n",
    "df = pd.read_csv(url, names=column_names)\n",
    "\n",
    "# Handle missing values\n",
    "df = df.replace('?', np.nan)\n",
    "df = df.dropna()\n",
    "\n",
    "# Encode categorical variables\n",
    "le = LabelEncoder()\n",
    "df['class'] = le.fit_transform(df['class'])\n",
    "\n",
    "# Split features and target variable\n",
    "X = df.drop(['id', 'class'], axis=1)\n",
    "y = df['class']\n",
    "\n",
    "# Convert X to numpy.ndarray type and change the data type to np.float32\n",
    "X_array = X.values.astype(np.float32)\n",
    "\n",
    "# Build a decision tree\n",
    "clf = DecisionTreeClassifier(min_samples_leaf=2, min_samples_split=5, max_depth=2)\n",
    "clf.fit(X_array, y)\n",
    "\n",
    "# Calculate the entropy, Gini index, and misclassification error of the first split\n",
    "dot_data = export_graphviz(clf, out_file=None, feature_names=X.columns, class_names=['benign','malignant'],\n",
    "                           filled=True, rounded=True, special_characters=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "\n",
    "# Get the node information of the first split\n",
    "left_child_index = clf.tree_.children_left[0]\n",
    "right_child_index = clf.tree_.children_right[0]\n",
    "\n",
    "# Calculate the total number of samples\n",
    "total_samples = len(y)\n",
    "\n",
    "# Calculate the number of samples in the left and right child nodes\n",
    "left_samples = clf.tree_.n_node_samples[left_child_index]\n",
    "right_samples = clf.tree_.n_node_samples[right_child_index]\n",
    "\n",
    "# Calculate the class distribution of the left and right child nodes\n",
    "left_class_counts = Counter(y[clf.tree_.apply(X_array) == left_child_index])\n",
    "right_class_counts = Counter(y[clf.tree_.apply(X_array) == right_child_index])\n",
    "\n",
    "# Calculate the entropy of the parent node\n",
    "parent_entropy = 0\n",
    "parent_class_counts = Counter(y)\n",
    "for count in parent_class_counts.values():\n",
    "    prob = count / total_samples\n",
    "    parent_entropy -= prob * math.log2(prob)\n",
    "\n",
    "# Calculate the entropy of the left child node\n",
    "left_entropy = 0\n",
    "if left_class_counts:\n",
    "    for count in left_class_counts.values():\n",
    "        prob = count / left_samples\n",
    "        left_entropy -= prob * math.log2(prob)\n",
    "\n",
    "# Calculate the entropy of the right child node\n",
    "right_entropy = 0\n",
    "if right_class_counts:\n",
    "    for count in right_class_counts.values():\n",
    "        prob = count / right_samples\n",
    "        right_entropy -= prob * math.log2(prob)\n",
    "\n",
    "# Calculate the entropy after the first split\n",
    "split_entropy = (left_samples / total_samples) * left_entropy + (right_samples / total_samples) * right_entropy\n",
    "\n",
    "# Calculate the information gain\n",
    "information_gain = parent_entropy - split_entropy\n",
    "\n",
    "# Calculate the Gini index\n",
    "parent_gini = 1\n",
    "for count in parent_class_counts.values():\n",
    "    prob = count / total_samples\n",
    "    parent_gini -= prob ** 2\n",
    "\n",
    "left_gini = 1\n",
    "if left_class_counts:\n",
    "    for count in left_class_counts.values():\n",
    "        prob = count / left_samples\n",
    "        left_gini -= prob ** 2\n",
    "else:\n",
    "    left_gini = 0\n",
    "\n",
    "right_gini = 1\n",
    "if right_class_counts:\n",
    "    for count in right_class_counts.values():\n",
    "        prob = count / right_samples\n",
    "        right_gini -= prob ** 2\n",
    "else:\n",
    "    right_gini = 0\n",
    "\n",
    "split_gini = (left_samples / total_samples) * left_gini + (right_samples / total_samples) * right_gini\n",
    "\n",
    "# Calculate the misclassification error\n",
    "parent_misclassification = 1 - max([count / total_samples for count in parent_class_counts.values()])\n",
    "left_misclassification = 0\n",
    "if left_class_counts:\n",
    "    left_misclassification = 1 - max([count / left_samples for count in left_class_counts.values()])\n",
    "\n",
    "right_misclassification = 0\n",
    "if right_class_counts:\n",
    "    right_misclassification = 1 - max([count / right_samples for count in right_class_counts.values()])\n",
    "\n",
    "split_misclassification = (left_samples / total_samples) * left_misclassification + (right_samples / total_samples) * right_misclassification\n",
    "\n",
    "print(f\"Entropy of the first split: {split_entropy}\")\n",
    "print(f\"Gini index of the first split: {split_gini}\")\n",
    "print(f\"Misclassification error of the first split: {split_misclassification}\")\n",
    "print(f\"Information gain: {information_gain}\")\n",
    "print(f\"Feature selected for the first split: {X.columns[clf.tree_.feature[0]]}\")\n",
    "print(f\"Value of the decision boundary: {clf.tree_.threshold[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cacb57bf-c978-44ca-a52a-16eb38f031c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the first principal component:\n",
      "F1 score: 0.8992248062015504, Precision: 0.8787878787878788, Recall: 0.9206349206349206\n",
      "False positives: 8, True positives: 58, False positive rate: 0.07407407407407407, True positive rate: 0.9206349206349206\n",
      "Using the first two principal components:\n",
      "F1 score: 0.8852459016393442, Precision: 0.9152542372881356, Recall: 0.8571428571428571\n",
      "False positives: 5, True positives: 54, False positive rate: 0.046296296296296294, True positive rate: 0.8571428571428571\n",
      "Using the original data:\n",
      "F1 score: 0.9047619047619048, Precision: 0.9047619047619048, Recall: 0.9047619047619048\n",
      "False positives: 6, True positives: 57, False positive rate: 0.05555555555555555, True positive rate: 0.9047619047619048\n",
      "The benefits of using continuous data may include retaining more information, thereby improving the model's performance. However, it may also increase computational complexity and the risk of overfitting. A trade-off needs to be made based on the specific situation.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data'\n",
    "column_names = ['id', 'diagnosis'] + [f'feature_{i}' for i in range(30)]\n",
    "df = pd.read_csv(url, names=column_names)\n",
    "\n",
    "# Encode the categorical variable\n",
    "df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0})\n",
    "\n",
    "# Split features and target variable\n",
    "X = df.drop(['id', 'diagnosis'], axis=1)\n",
    "y = df['diagnosis']\n",
    "\n",
    "# Split the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Data standardization\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Dimensionality reduction with PCA\n",
    "pca = PCA()\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Use the first principal component\n",
    "X_train_pca_1 = X_train_pca[:, :1]\n",
    "X_test_pca_1 = X_test_pca[:, :1]\n",
    "\n",
    "clf_pca_1 = DecisionTreeClassifier(min_samples_leaf=2, min_samples_split=5, max_depth=2)\n",
    "clf_pca_1.fit(X_train_pca_1, y_train)\n",
    "y_pred_pca_1 = clf_pca_1.predict(X_test_pca_1)\n",
    "\n",
    "f1_pca_1 = f1_score(y_test, y_pred_pca_1)\n",
    "precision_pca_1 = precision_score(y_test, y_pred_pca_1)\n",
    "recall_pca_1 = recall_score(y_test, y_pred_pca_1)\n",
    "\n",
    "# Use the first two principal components\n",
    "X_train_pca_2 = X_train_pca[:, :2]\n",
    "X_test_pca_2 = X_test_pca[:, :2]\n",
    "\n",
    "clf_pca_2 = DecisionTreeClassifier(min_samples_leaf=2, min_samples_split=5, max_depth=2)\n",
    "clf_pca_2.fit(X_train_pca_2, y_train)\n",
    "y_pred_pca_2 = clf_pca_2.predict(X_test_pca_2)\n",
    "\n",
    "f1_pca_2 = f1_score(y_test, y_pred_pca_2)\n",
    "precision_pca_2 = precision_score(y_test, y_pred_pca_2)\n",
    "recall_pca_2 = recall_score(y_test, y_pred_pca_2)\n",
    "\n",
    "# Use the original data\n",
    "clf_original = DecisionTreeClassifier(min_samples_leaf=2, min_samples_split=5, max_depth=2)\n",
    "clf_original.fit(X_train_scaled, y_train)\n",
    "y_pred_original = clf_original.predict(X_test_scaled)\n",
    "\n",
    "f1_original = f1_score(y_test, y_pred_original)\n",
    "precision_original = precision_score(y_test, y_pred_original)\n",
    "recall_original = recall_score(y_test, y_pred_original)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_pca_1 = confusion_matrix(y_test, y_pred_pca_1)\n",
    "cm_pca_2 = confusion_matrix(y_test, y_pred_pca_2)\n",
    "cm_original = confusion_matrix(y_test, y_pred_original)\n",
    "\n",
    "# False positive rate and true positive rate\n",
    "FP_pca_1 = cm_pca_1[0, 1]\n",
    "TP_pca_1 = cm_pca_1[1, 1]\n",
    "FPR_pca_1 = FP_pca_1 / (cm_pca_1[0, 0] + FP_pca_1)\n",
    "TPR_pca_1 = TP_pca_1 / (cm_pca_1[1, 0] + TP_pca_1)\n",
    "\n",
    "FP_pca_2 = cm_pca_2[0, 1]\n",
    "TP_pca_2 = cm_pca_2[1, 1]\n",
    "FPR_pca_2 = FP_pca_2 / (cm_pca_2[0, 0] + FP_pca_2)\n",
    "TPR_pca_2 = TP_pca_2 / (cm_pca_2[1, 0] + TP_pca_2)\n",
    "\n",
    "FP_original = cm_original[0, 1]\n",
    "TP_original = cm_original[1, 1]\n",
    "FPR_original = FP_original / (cm_original[0, 0] + FP_original)\n",
    "TPR_original = TP_original / (cm_original[1, 0] + TP_original)\n",
    "\n",
    "print(\"Using the first principal component:\")\n",
    "print(f\"F1 score: {f1_pca_1}, Precision: {precision_pca_1}, Recall: {recall_pca_1}\")\n",
    "print(f\"False positives: {FP_pca_1}, True positives: {TP_pca_1}, False positive rate: {FPR_pca_1}, True positive rate: {TPR_pca_1}\")\n",
    "\n",
    "print(\"Using the first two principal components:\")\n",
    "print(f\"F1 score: {f1_pca_2}, Precision: {precision_pca_2}, Recall: {recall_pca_2}\")\n",
    "print(f\"False positives: {FP_pca_2}, True positives: {TP_pca_2}, False positive rate: {FPR_pca_2}, True positive rate: {TPR_pca_2}\")\n",
    "\n",
    "print(\"Using the original data:\")\n",
    "print(f\"F1 score: {f1_original}, Precision: {precision_original}, Recall: {recall_original}\")\n",
    "print(f\"False positives: {FP_original}, True positives: {TP_original}, False positive rate: {FPR_original}, True positive rate: {TPR_original}\")\n",
    "\n",
    "# Discuss the benefits of using continuous data\n",
    "print(\"The benefits of using continuous data may include retaining more information, thereby improving the model's performance. However, it may also increase computational complexity and the risk of overfitting. A trade-off needs to be made based on the specific situation.\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
